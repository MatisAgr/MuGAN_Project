"""
Script d'entra√Ænement d'un mod√®le de g√©n√©ration musicale avec TensorFlow/Keras.
Entra√Æne un mod√®le sur les s√©quences de notes MIDI pr√©trait√©es.
"""

import os
import json
import argparse
from pathlib import Path
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# Configuration TensorFlow
tf.config.set_visible_devices([], 'GPU')  # Force CPU si GPU probl√©matique
tf.data.experimental.enable_debug_mode()  # Mode debug pour meilleur diagnostique

# D√©terminer le dossier du projet (parent du dossier src)
PROJECT_DIR = Path(__file__).parent.parent
DATA_PROCESSED_DIR = PROJECT_DIR / "data" / "processed"
MODELS_DIR = PROJECT_DIR / "models" / "music_vae"


def build_model(sequence_length: int, vocab_size: int = 128) -> keras.Model:
    """
    Construit un mod√®le RNN simple pour la g√©n√©ration musicale.
    
    Args:
        sequence_length: Longueur des s√©quences d'entr√©e
        vocab_size: Nombre de notes uniques (0-127 pour MIDI)
        
    Returns:
        Mod√®le Keras compil√©
    """
    model = keras.Sequential([
        # Couche d'embedding pour repr√©senter les notes
        layers.Embedding(vocab_size + 2, 64),
        
        # LSTM pour capturer les d√©pendances
        layers.LSTM(128, return_sequences=True),
        layers.Dropout(0.2),
        
        # LSTM suppl√©mentaire
        layers.LSTM(64, return_sequences=False),
        layers.Dropout(0.2),
        
        # Couches fully connected
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),
        
        # Output layer - pr√©dire la prochaine note
        layers.Dense(vocab_size + 2, activation='softmax')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model


def prepare_data(sequences: np.ndarray, sequence_length: int):
    """
    Pr√©pare les donn√©es pour l'entra√Ænement.
    Cr√©e des paires (input, target) √† partir des s√©quences.
    Optimis√© pour limiter l'utilisation m√©moire.
    
    Args:
        sequences: Array de s√©quences (num_sequences, sequence_length)
        sequence_length: Longueur des s√©quences
        
    Returns:
        Tuple (X, y) pr√™ts pour l'entra√Ænement
    """
    X = []
    y = []
    
    for seq in sequences:
        # Prendre la s√©quence enti√®re comme input, pr√©dire la derni√®re note
        X.append(seq)
        y.append(seq[-1])
    
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.int32)
    
    # Remplacer les valeurs -1 (silence) par un index sp√©cial (128)
    X = np.where(X == -1, 128, X).astype(np.int32)
    # Les targets peuvent aussi contenir -1, remplacer par 128
    y = np.where(y == -1, 128, y).astype(np.int32)
    
    return X, y


def save_metrics_json(history, model_dir: str, num_epochs: int, batch_size: int):
    """
    Sauvegarde les m√©triques d'entra√Ænement en format JSON.
    √âcrase le fichier pr√©c√©dent √† chaque ex√©cution.
    
    Args:
        history: Objet History retourn√© par model.fit()
        model_dir: Dossier o√π sauvegarder le fichier JSON
        num_epochs: Nombre d'epochs d'entra√Ænement
        batch_size: Taille des batches utilis√©e
    """
    metrics_path = os.path.join(model_dir, "training_metrics.json")
    
    metrics = {
        "num_epochs": num_epochs,
        "batch_size": batch_size,
        "loss": history.history.get('loss', []),
        "accuracy": history.history.get('accuracy', []),
        "val_loss": history.history.get('val_loss', []),
        "val_accuracy": history.history.get('val_accuracy', []),
        "final_metrics": {
            "loss": float(history.history['loss'][-1]),
            "accuracy": float(history.history['accuracy'][-1]),
            "val_loss": float(history.history['val_loss'][-1]),
            "val_accuracy": float(history.history['val_accuracy'][-1]),
            "epochs_trained": len(history.history['loss'])
        }
    }
    
    # Convertir les listes numpy en listes Python (pour JSON)
    for key in metrics:
        if isinstance(metrics[key], list):
            metrics[key] = [float(x) for x in metrics[key]]
    
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"üíæ M√©triques sauvegard√©es: {metrics_path}")
    
    return metrics_path



def train_model(train_dir: str,
                model_dir: str,
                num_epochs: int = 20,
                batch_size: int = 32,
                sequence_length: int = 32):
    """
    Entra√Æne le mod√®le de g√©n√©ration musicale.
    
    Args:
        train_dir: Dossier contenant les donn√©es pr√©trait√©es
        model_dir: Dossier pour sauvegarder le mod√®le
        num_epochs: Nombre d'epochs d'entra√Ænement
        batch_size: Taille des batches
        sequence_length: Longueur des s√©quences
    """
    os.makedirs(model_dir, exist_ok=True)
    
    print("=" * 60)
    print("üéì ENTRA√éNEMENT DU MOD√àLE DE G√âN√âRATION MUSICALE")
    print("=" * 60)
    
    # Charger les donn√©es
    print("\nüìÇ Chargement des donn√©es...")
    train_path = os.path.join(train_dir, "train_sequences.npy")
    val_path = os.path.join(train_dir, "validation_sequences.npy")
    
    if not os.path.exists(train_path):
        print(f"‚ùå Fichier non trouv√©: {train_path}")
        print("   Avez-vous ex√©cut√© preprocess.py d'abord?")
        return
    
    train_sequences = np.load(train_path)
    val_sequences = np.load(val_path)
    
    print(f"‚úÖ Donn√©es d'entra√Ænement charg√©es: {train_sequences.shape}")
    print(f"‚úÖ Donn√©es de validation charg√©es: {val_sequences.shape}")
    
    # Pr√©parer les donn√©es
    print("\nüîÑ Pr√©paration des donn√©es...")
    X_train, y_train = prepare_data(train_sequences, sequence_length)
    X_val, y_val = prepare_data(val_sequences, sequence_length)
    
    print(f"‚úÖ X_train shape: {X_train.shape}")
    print(f"‚úÖ y_train shape: {y_train.shape}")
    
    # Construire le mod√®le
    print("\nüèóÔ∏è Construction du mod√®le...")
    model = build_model(sequence_length)
    print("‚úÖ Mod√®le construit!")
    
    # Afficher le r√©sum√© du mod√®le
    print("\nüìä R√©sum√© du mod√®le:")
    model.summary()
    
    # Callbacks
    checkpoint = keras.callbacks.ModelCheckpoint(
        os.path.join(model_dir, 'best_model.h5'),
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    )
    
    early_stop = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=3,
        verbose=1
    )
    
    # Entra√Æner le mod√®le
    print("\nüöÄ D√©marrage de l'entra√Ænement...")
    history = model.fit(
        X_train, y_train,
        epochs=num_epochs,
        batch_size=batch_size,
        validation_data=(X_val, y_val),
        callbacks=[checkpoint, early_stop],
        verbose=1
    )
    
    # Sauvegarder le mod√®le final
    final_model_path = os.path.join(model_dir, 'model_final.h5')
    model.save(final_model_path)
    print(f"\nüíæ Mod√®le sauvegard√©: {final_model_path}")
    
    # Sauvegarder l'architecture
    with open(os.path.join(model_dir, 'model_config.json'), 'w') as f:
        f.write(model.to_json())
    
    # Tracer les courbes d'apprentissage
    print("\nÔøΩ G√©n√©ration des graphiques...")
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Perte lors de l\'entra√Ænement')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Pr√©cision lors de l\'entra√Ænement')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    graph_path = os.path.join(model_dir, 'training_history.png')
    plt.savefig(graph_path, dpi=100, bbox_inches='tight')
    print(f"‚úÖ Graphiques sauvegard√©s: {graph_path}")
    
    # Sauvegarder les m√©triques en JSON (√©crase le fichier pr√©c√©dent)
    print("\nüìä Sauvegarde des m√©triques d'entra√Ænement...")
    save_metrics_json(history, model_dir, num_epochs, batch_size)
    
    print("\n" + "=" * 60)
    print("‚úÖ ENTRA√éNEMENT TERMIN√â!")
    print(f"üìÅ Mod√®les sauvegard√©s dans: {model_dir}")
    print("=" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Entra√Ænement du mod√®le de g√©n√©ration musicale")
    parser.add_argument("--train_dir", type=str, default=str(DATA_PROCESSED_DIR),
                        help="Dossier contenant les donn√©es pr√©trait√©es")
    parser.add_argument("--model_dir", type=str, default=str(MODELS_DIR),
                        help="Dossier pour sauvegarder le mod√®le")
    parser.add_argument("--num_epochs", type=int, default=20,
                        help="Nombre d'epochs")
    parser.add_argument("--batch_size", type=int, default=32,
                        help="Taille des batches")
    parser.add_argument("--sequence_length", type=int, default=32,
                        help="Longueur des s√©quences")
    
    args = parser.parse_args()
    
    print(f"üìÅ Dossier du projet: {PROJECT_DIR}")
    print(f"üìÅ Donn√©es d'entra√Ænement: {args.train_dir}")
    print(f"üìÅ Mod√®les: {args.model_dir}\n")
    
    train_model(
        train_dir=args.train_dir,
        model_dir=args.model_dir,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        sequence_length=args.sequence_length
    )

